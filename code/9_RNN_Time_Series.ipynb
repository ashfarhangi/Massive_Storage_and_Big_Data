{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "9_RNN_Time_Series.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfarhangi/Massive_Storage_and_Big_Data/blob/master/code/9_RNN_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd24llZ5RqJ9"
      },
      "source": [
        "# STL Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw71XxIONFYA"
      },
      "source": [
        "!pip install statsmodels==0.12.1\n",
        "from statsmodels.tsa.seasonal import STL \n",
        "from statsmodels.datasets import co2\n",
        "data = co2.load(True).data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1h1opfRS0w"
      },
      "source": [
        "data = data.resample('M').mean().ffill()\n",
        "res = STL(data).fit()\n",
        "res.plot()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGW0B417OeBo"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSsjmBLzOe-O"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "df = pd.read_csv(csv_path)\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jia3JScvPxfs"
      },
      "source": [
        "df1 = df.copy()\n",
        "df1 = df1.set_index('Date Time')\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr38iJQZPAtS"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK9ZT5MxPFuq"
      },
      "source": [
        "df1['T (degC)'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIOGAv1wPtfb"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeITAiRWQDb3"
      },
      "source": [
        "df1[['T (degC)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p59CEFUxQQNl"
      },
      "source": [
        "df1[['T (degC)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPK4fYUAQesY"
      },
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(df1[['T (degC)'][:1000]], freq=30)\n",
        "\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "decomposition.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-o9n2YRPlPa"
      },
      "source": [
        "from statsmodels.datasets import co2\n",
        "data = co2.load(True).data\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYZjSI6NApD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig-9bz4tNSCR"
      },
      "source": [
        "# Time Series Prediction (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMNBWOL0BpE"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rZnJaGTWQw0"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyv_i85IWInT"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6uGeeeWIkG"
      },
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WIQcVGnbS-"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia-MPAHxbInX"
      },
      "source": [
        "TRAIN_SPLIT = 300000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlJYi3_HXcw8"
      },
      "source": [
        "## Part 2: Forecast a multivariate time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxNZ2GM7DPm"
      },
      "source": [
        "The original dataset contains fourteen features. For simplicity, this section considers only three of the original fourteen. The features used are air temperature, atmospheric pressure, and air density. \n",
        "\n",
        "To use more features, add their names to this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DphrB7bxSNDd"
      },
      "source": [
        "tf.random.set_seed(13)\n",
        "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQUSiJfUpXJ"
      },
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['Date Time']\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfhTZi5r15R"
      },
      "source": [
        "Let's have a look at how each of these features vary across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdgC8zvGr21X"
      },
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqStgZ-O1b3_"
      },
      "source": [
        "As mentioned, the first step will be to standardize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VuNIwfHRHx"
      },
      "source": [
        "dataset = features.values\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJUeWDqploCt"
      },
      "source": [
        "dataset = (dataset-data_mean)/data_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y09cNql5X4GV"
      },
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i, step)\n",
        "    data.append(dataset[indices])\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sYQ9w5oX7w8"
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GnE087bJYSu"
      },
      "source": [
        "### Multi-Step model\n",
        "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predict a sequence of the future.\n",
        "\n",
        "For the multi-step model, the training data again consists of recordings over the past five days sampled every hour. However, here, the model needs to learn to predict the temperature for the next 12 hours. Since an obversation is taken every 10 minutes, the output is 72 predictions. For this task, the dataset needs to be prepared accordingly, thus the first step is just to create it again, but with a different target window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCk9fqyJZqX"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "future_target = 72\n",
        "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)\n",
        "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
        "print ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))\n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Viwjuq2oZbVd"
      },
      "source": [
        "def create_time_steps(length):\n",
        "  return list(range(-length, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXKVbwBV7D3"
      },
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQKetflZRMF"
      },
      "source": [
        "In this plot and subsequent similar plots, the history and the future data are sampled every hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6G8bacQR4w2"
      },
      "source": [
        "for x, y in train_data_multi.take(1):\n",
        "  multi_step_plot(x[0], y[0], np.array([0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOjz8DzZ4HFS"
      },
      "source": [
        "Since the task here is a bit more complicated than the previous task, the model now consists of two LSTM layers. Finally, since 72 predictions are made, the dense layer outputs 72 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byAl0NKSNBP6"
      },
      "source": [
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=x_train_multi.shape[-2:]))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.Dense(72))\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXVoV81NYu8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13_ZWvB9SRlZ"
      },
      "source": [
        "for x, y in val_data_multi.take(1):\n",
        "  print (multi_step_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uwOhXo3Oems"
      },
      "source": [
        "EPOCHS = 10\n",
        "EVALUATION_INTERVAL = 200\n",
        "\n",
        "\n",
        "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDg94-yq4pas"
      },
      "source": [
        "#### Predict a multi-step future\n",
        "Let's now have a look at how well your network has learnt to predict the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt22wq6fyIBU"
      },
      "source": [
        "for x, y in val_data_multi.take(3):\n",
        "  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqXDN7isNfaD"
      },
      "source": [
        "## Takehome Exercise:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GGgt7nyNriA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzMXEORpNiOD"
      },
      "source": [
        "\n",
        "1.   Improve the accuracy of prediction by choosing different hyper parameters (Hint: LSTM(64))\n",
        "2.   Use the STL decomposition as features for the network and showcase the improvement \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqwbThLiRCuZ"
      },
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(df[['T (degC)']], freq=30)\n",
        "\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "decomposition.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCRdaMu4FM9h"
      },
      "source": [
        "# Reference\n",
        "https://www.tensorflow.org/tutorials/structured_data/time_series"
      ]
    }
  ]
}