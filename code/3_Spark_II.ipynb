{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Spark_II.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfarhangi/Massive_Storage_and_Big_Data/blob/master/code/3_Spark_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpnnbb7yQlNe",
        "colab_type": "text"
      },
      "source": [
        "# Spark II\n",
        "MapReduce, Spark, Real-World example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do1U0yxBG4q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar -xvf /content/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "# Start A Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "words = [\"Hello\", \"Spark\", \"I am here\",'Happe','Spark']\n",
        "sc.parallelize(words).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhp1h9qYVSOg",
        "colab_type": "text"
      },
      "source": [
        "# Part A\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoFg4aAHIXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLOUmmXwHPUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A helper function to compute the list of words in a line of text\n",
        "import re\n",
        "def get_words(line):\n",
        "    return re.compile('\\w+').findall(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39e1FSC_r369",
        "colab_type": "text"
      },
      "source": [
        "## WordCount\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVBLkDgIQMdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ashfarhangi/Reinforcement-Learning-Trading/master/meta.txt\n",
        "!wget -q https://raw.githubusercontent.com/ashfarhangi/Reinforcement-Learning-Trading/master/review.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrN8gF8UCPmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6fd5e51-a704-4ab2-bcc0-f585ef97b3d7"
      },
      "source": [
        "#Test.txt\n",
        "#function to take input test file and make lower case\n",
        "def Func(lines):\n",
        "      lines = lines.lower()\n",
        "      return lines\n",
        "\n",
        "#read test file, split into different lines for analyzation\n",
        "#change test file\n",
        "lines1 = sc.textFile(\"test.txt\").flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "#apply function to make lowercase\n",
        "fixed_line = lines1.map(Func)\n",
        "\n",
        "#Read user input, make lower case\n",
        "WordIn = input(\"Word: \").lower()\n",
        "\n",
        "#count instance of word in each line\n",
        "CountFind = fixed_line.filter(lambda line: WordIn in get_words(line)).count()\n",
        "\n",
        "#print result, which is case insensitive\n",
        "print(CountFind)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: TeSt\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkDdhzUVaZp",
        "colab_type": "text"
      },
      "source": [
        "To analyse large datasets using Spark you will load them into Resilient Distributed Datasets (RDDs). There are a number of ways in which you can create RDDs. Use the `parallelize()` function to create one from a Python collection, and use the `textFile()` function to create an RDD from the file `review.txt`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-MP9qaBsVQh",
        "colab_type": "text"
      },
      "source": [
        "## Finding Keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "42H-mxkbE-zL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4b5ac9b-5b38-4d5c-a2a2-7d7c7be769f5"
      },
      "source": [
        "#Question 2\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#read it meta and review text files\n",
        "sqlContext = SQLContext(sc)\n",
        "dfmeta = sqlContext.read.json('meta.txt')\n",
        "dfdata = sqlContext.read.json('review.txt')\n",
        "\n",
        "#part 1\n",
        "###########################################################################################\n",
        "#add the number of words in a new col\n",
        "filterDFdata_word = dfdata.withColumn('wordCount', f.size(f.split(f.col('reviewText'), ' ')))\n",
        "\n",
        "#filter by the number of words greater or equal to 100, order by overall rating\n",
        "filterDFdata = filterDFdata_word.filter(filterDFdata_word.wordCount > \"100\").orderBy(\"overall\")\n",
        "\n",
        "#print and save output to file\n",
        "filterDFdata.coalesce(1).write.csv(\"/content/overview_output.csv\")\n",
        "\n",
        "########\n",
        "\n",
        "#part 2\n",
        "###########################################################################################\n",
        "#filter by music category\n",
        "filterDFmeta = dfmeta.filter(dfmeta.categories == \"Music\")\n",
        "\n",
        "#match the filtered music category to asin numbers to get review size\n",
        "diff_data = filterDFmeta.join(dfdata,filterDFmeta[\"asin\"] == dfdata[\"asin\"],\"left\")\n",
        "\n",
        "#add a col with the number of words in the review\n",
        "wordcnt = diff_data.withColumn('wordCount', f.size(f.split(f.col('reviewText'), ' ')))\n",
        "\n",
        "#count the number of rows to devide from the total number of words in all reviews\n",
        "x = diff_data.count()   \n",
        "\n",
        "#Count total number of words in the columns\n",
        "Sumcol = wordcnt.agg(f.sum(\"wordCount\")).collect()[0][0]\n",
        "\n",
        "#take the total number of words and devide by the number of rows\n",
        "Average_word = Sumcol / x\n",
        "\n",
        "#Total Average\n",
        "print(Average_word)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94.2723756312453\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}